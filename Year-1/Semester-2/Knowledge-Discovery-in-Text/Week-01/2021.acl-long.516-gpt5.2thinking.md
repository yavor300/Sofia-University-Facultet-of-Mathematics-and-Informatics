Below is a **GitHub README–style Markdown** you can copy into a `README.md` for your project notes, based on the paper **“Detecting Propaganda Techniques in Memes” (ACL 2021)**. 

---

# Tech & Methods Notes — Detecting Propaganda Techniques in Memes (ACL 2021)

**Paper:** *Detecting Propaganda Techniques in Memes* (Dimitrov et al., 2021). 
**Task:** Multi-label, multimodal classification — given a meme (image + embedded text), predict **which propaganda techniques** are present.

## Why this paper matters for a project

Modern propaganda online is often **multimodal** (text + image). The paper shows that models that jointly reason over both modalities outperform text-only or image-only approaches, and it provides an annotation recipe you can reuse. 

---

## Core technologies & building blocks

### 1) Multimodal propaganda technique taxonomy (22 labels)

They use **22 propaganda techniques**, adapted from prior propaganda/fallacy inventories. Most are applicable to both text and image; two are image-only in their setup. 

**Examples of technique categories (non-exhaustive):**

* Loaded language, Name calling/Labeling, Smears
* Appeal to authority, Straw man, Whataboutism
* Causal oversimplification, Red herring
* Bandwagon, Repetition, Slogans
* Image-focused: **Transfer**, **Appeal to (Strong) Emotions** 

**Implementation tip:** model it as **multi-label classification** (`sigmoid` per label), not softmax.

---

### 2) Dataset construction pipeline (reusable process)

**Dataset size:** 950 memes, manually annotated with 22 techniques. 

**Collection strategy**

* Sourced from diverse Facebook groups/topics (politics, vaccines, COVID-19, etc.) over time to cover variety. 

**Text extraction**

* OCR used to extract embedded text; they specifically mention **Google Vision API** for OCR + manual post-editing to fix OCR errors. 

**Annotation platform**

* **PyBossa** used to build a custom annotation interface. 

---

### 3) Annotation workflow (5 phases)

A high-quality, debate-and-consolidate workflow designed to reduce subjectivity. 

1. **Filtering + OCR text post-editing**
2. **Text-only span annotation** (choose technique + highlight spans)
3. **Text consolidation** (group discussion to reach consensus)
4. **Multimodal annotation** (image + text; starts from consolidated text labels)
5. **Multimodal consolidation** (final consensus)

**Key idea to reuse:** separate *text-only understanding* from *multimodal understanding*, then reconcile via consolidation. 

---

### 4) Quality measurement for multi-label annotation

They evaluate annotator agreement with **Krippendorff’s α**, adapted to multi-label settings, and report moderate→high agreement overall. 

---

## Modeling approaches used (and what to reuse)

### Baselines

* Random baseline
* Majority-class baseline (always predict most frequent technique) 

### Unimodal models

**Text-only**

* **BERT** (contextual transformer)
* **fastText** (word + character n-grams; robust to noisy social text) 

**Image-only**

* **ResNet-152** 

### Multimodal fusion strategies (important)

They compare several ways to combine modalities:

* **Late fusion:** average/merge predictions from text model + image model
* **Mid fusion:** concatenate intermediate representations (e.g., BERT [CLS] + ResNet penultimate layer) then classify
* **Early fusion / joint multimodal transformers:** architectures designed to attend across image regions and text jointly

### Joint multimodal transformer models

* **MMBT (Multimodal Bitransformers)**
* **ViLBERT**
* **VisualBERT** 

**Finding to reuse:** joint multimodal transformers outperform unimodal models and simple fusion in this task. 

---

## Training & evaluation setup

### Data split

Train / Dev / Test: **687 / 63 / 200** examples. 

### Metrics for multi-label imbalance

They focus on **micro-F1** (and also report macro-F1). 

Micro-precision/recall/F1 (multi-label) reminder:

$
P_{\text{micro}} = \frac{\sum_\ell TP_\ell}{\sum_\ell (TP_\ell + FP_\ell)}, \quad
R_{\text{micro}} = \frac{\sum_\ell TP_\ell}{\sum_\ell (TP_\ell + FN_\ell)}, \quad
F1_{\text{micro}} = \frac{2 P_{\text{micro}} R_{\text{micro}}}{P_{\text{micro}} + R_{\text{micro}}}
$

### Framework & hyperparameters (practical defaults)

* **MMF (Multimodal Framework)** used for experiments
* Batch size 32, AdamW, LR (5 \times 10^{-5}), weight decay 0.01, max seq length 128, early stopping on dev micro-F1 

---

## Design patterns you can directly port to your project

### A) “Explainable” outputs via label + evidence spans

Even though their main experiments predict labels at meme-level, their annotation includes **text spans**, enabling:

* evidence highlighting (what phrase triggers which technique)
* UI overlays for moderation/analysis tools 

### B) Multimodal labeling that respects context dependence

Some techniques only become detectable when the **image disambiguates the text** (or vice versa). Their Phase 4 design explicitly targets this. 

### C) Build your dataset with “human-in-the-loop consolidation”

The consolidation stages (3 and 5) are a strong recipe for higher-quality labels in subjective tasks. 

---

## Project ideas you can build from this

### 1) “Propaganda Technique Detector” web app (meme upload → labels + evidence)

**User story:** Upload a meme → get predicted techniques + highlighted text spans + confidence per technique.
**Extensions:**

* “Why?” panel: show top tokens/regions contributing (Grad-CAM for image + attention/attribution for text)
* Compare unimodal vs multimodal predictions (what the image adds)

### 2) Meme monitoring dashboard (for journalists / moderators)

**Pipeline:**

* ingest posts (images) → OCR → multimodal classifier → aggregate stats
  **Features:**
* trending techniques over time
* technique co-occurrence graph (e.g., Smears + Loaded language pairs are common) 

### 3) Dataset + annotation tool starter kit

Recreate their 5-phase workflow:

* OCR module (Google Vision / open-source OCR)
* annotation UI (PyBossa-like or your own)
* consolidation workflow support (diffing spans, conflict resolution)

### 4) Cross-lingual extension project

They explicitly mention future extension to other languages. You can:

* translate/adapt technique guidelines
* evaluate multilingual multimodal models
* compare technique distributions by language/community 

### 5) “Counter-propaganda rewriting assistant”

Given detected techniques, generate:

* neutral rewrite of the meme text
* suggestions like “replace loaded language”, “avoid false dichotomy”
  (Useful as an educational tool.)

---

## Suggested implementation stack (practical)

* **OCR:** Google Vision API (as in the paper) or Tesseract/EasyOCR
* **Text encoder:** BERT-family (or RoBERTa/DeBERTa depending on your setup)
* **Image encoder:** ResNet / ViT
* **Multimodal:** VisualBERT / ViLBERT / CLIP-based fusion
* **Training:** PyTorch + HuggingFace + MMF-style data loaders
* **Evaluation:** micro-F1 primary; add per-label PR curves if you need analysis

---

## Reference

* Dimitrov et al. (2021) *Detecting Propaganda Techniques in Memes*. ACL 2021. 

---

If you want, paste what kind of project you’re building (research prototype, moderation tool, school project, etc.) and I’ll tailor this README to a concrete architecture (folders, data schema, model choices, and milestones).
