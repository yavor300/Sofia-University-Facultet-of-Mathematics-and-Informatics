# Technologies and Methods for Check-Worthiness Prediction

This document summarizes the methodology and architectural techniques described in the paper *"It Takes Nine to Smell a Rat: Neural Multi-Task Learning for Check-Worthiness Prediction"*.

## 1. Model Architecture
The authors propose a **Neural Multi-Task Learning (MTL)** approach to estimate the check-worthiness of claims in political debates.

* **Hard Parameter Sharing:** The architecture uses a shared hidden layer for all tasks to reduce the risk of overfitting.
* **Layer Configuration:**
    * **Input Layer:** Accepts various domain-specific features.
    * **Hard-Shared Layer:** A hidden layer with 300 ReLU units shared between all tasks.
    * **Task-Specific Hidden Layers:** Ten parallel layers (300 ReLU units each) corresponding to nine individual media sources and one cumulative "ANY" task.
    * **Output Layer:** A single sigmoid unit for each task providing the prediction probability.

## 2. Feature Engineering
The model integrates a comprehensive set of features categorized into several groups:

| Feature Group | Description |
| :--- | :--- |
| **Embeddings** | Word embeddings pre-trained on Google News. |
| **Linguistic / NLP** | TF.IDF-weighted bag of words, part-of-speech (POS) tags, named entities (NER), and sentence length. |
| **Lexicon** | Features for bias, sentiment, assertiveness, and subjectivity. |
| **Discourse** | Relations with respect to neighboring sentences. |
| **Structural** | Location of the sentence within the debate/intervention. |
| **Topics** | LDA (Latent Dirichlet Allocation) topics. |
| **Similarity** | Similarity to previously checked claims. |

## 3. Training and Optimization
* **Optimizer:** Stochastic Gradient Descent with Nesterov momentum.
* **Epochs:** Iterated for 100 epochs.
* **Validation:** 4-fold cross-validation, leaving one debate out for testing in each fold.
* **Stability:** Each experiment is repeated three times with different random seeds to report the average.

## 4. Evaluation Metrics
The task is approached as a ranking problem, using information retrieval measures:
* **Mean Average Precision (MAP):** The primary evaluation measure.
* **Precision at k (P@k):** Reported for $k=\{5, 10, 20, 50\}$.
* **R-Precision (R-Pr):** Precision at the number of relevant items.

## 5. Key Implementation Insights
* **Benefit of Multi-Tasking:** Learning from multiple sources (PolitiFact, CNN, NYT, etc.) simultaneously improves performance even when targeting a specific source.
* **Feature Importance:** Embeddings and Metadata features were found to be the most critical contributors to the model's performance.
* **Source Influence:** Removing certain sources (like NYT) from training can worsen results for other sources, indicating important shared relations.