# ClaimRank-inspired Tech & Techniques (for reuse)

This README distills the **technologies, components, and practical methods** described in the *ClaimRank: Detecting Check-Worthy Claims in Arabic and English* demo paper (NAACL-HLT 2018). 

---

## What the system does

**Goal:** rank sentences in an input text by “check-worthiness” (i.e., which claims a fact-checker should verify first). The output is a **score per sentence** and a **sorted view** of top candidates. 

---

## End-to-end architecture (pipeline)

### 1) Web app + server session flow

* **Web UI** where the user submits text and sees sentence-level scores with color intensity, plus sorting options. 
* The server:

  1. **Detects language** (English/Arabic) using `langdetect`. 
  2. **Splits into sentences** (NLTK for English; custom regex splitter for Arabic). 
  3. Stores sentence list + scores in a **session** (JSONified arrays) to support re-ordering and re-scoring without re-uploading the text. 
  4. Extracts sentence features → runs model → returns **check-worthiness score** per sentence. 

### 2) “Mimic a fact-checker outlet”

* The system is trained from annotations of **9 fact-checking organizations** and can mimic:

  * each individual outlet’s selection strategy, or
  * the union of them all. 

---

## Text processing techniques

### Language identification

* `langdetect` decides which processing stack to use. 

### Sentence splitting

* **English:** `NLTK sent_tokenize`. 
* **Arabic:** custom **regular-expression sentence splitter** to handle Arabic punctuation differences. 

### Tokenization & segmentation

* **English:** NLTK tokenizer. 
* **Arabic:** Farasa **segmenter** (tokenization alone is insufficient due to clitics/conjunctions attached to words, increasing sparsity). 

### POS tagging + tagset normalization

* **English POS:** NLTK tagger (Penn Treebank tags). 
* **Arabic POS:** Farasa tagger (Farasa tagset). 
* **Unification step:** map both tagsets to the **Universal POS tagset** to share features across languages. 

---

## Feature engineering (sentence-level)

ClaimRank reuses feature families shown effective for check-worthiness. 

### “Classic NLP” features

* **TF–IDF bag-of-words**. 
* **POS tag features**. 
* **Named entities** (paper mentions Alchemy API for NER). 
* **Sentiment scores**. 
* **Sentence length** (token count). 

### Lexicon-based features (handcrafted dictionaries)

* **Bias** lexicons. 
* **Sentiment** lexicons. 
* **Assertiveness** lexicons. 
* **Subjectivity** lexicons. 

### Structural + topical semantics

* **Sentence position** within the debate/intervention (structure/metadata-like feature). 
* **Topic modeling (LDA topics)**. 
* **Word embeddings** (dense semantic signals). 
* **Discourse relations** with neighboring sentences are mentioned as a feature family, but later they note discourse parsing was excluded in the cross-lingual setup due to lack of an Arabic discourse parser. 

---

## Model & ranking strategy

### Neural ranking/classification model

* A **feed-forward neural network** with **two hidden layers**:

  * 200 ReLU units,
  * 50 ReLU units,
  * final **sigmoid** output for check-worthy vs. not check-worthy. 
* Training uses **Stochastic Gradient Descent** for **100 iterations**. 

### Scoring and ranking

* Ranking is done by the model’s **predicted probability** of the positive class (check-worthy). 

If you want to describe this in math in your project docs, a typical sigmoid-based probability is:

$$
p(y=1 \mid \mathbf{x}) = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

where $z$ is the network’s final (pre-sigmoid) logit.

---

## Cross-lingual adaptation (Arabic + English)

### Practical adaptations to support Arabic

* Added language detection to route to the right tokenizer/sentence splitter. 
* Used Farasa for **segmentation** + **POS** for Arabic. 
* Unified POS feature space by mapping to **Universal POS**. 

### Cross-lingual embeddings (key technique)

They treat embeddings as one of the most important “portable” components across languages and evaluate multiple approaches: 

* **VecMap**

  * Train monolingual word2vec embeddings (EN + AR) on a parallel corpus (TED talks),
  * then project into a shared space via VecMap. 

* **MUSE (supervised)**

  * Similar setup (TED-based monolingual embeddings),
  * project to shared space using supervised MUSE. 

* **Attract–Repel (pretrained cross-lingual)**

  * Use pretrained EN–AR vectors. 

**Final choice in the deployed system:** **MUSE vectors for both languages**, reported as best overall in their evaluation. 

---

## Evaluation setup & metrics to reuse

### Train/test setup

* Train on **five** English debates, test on **two** debates (English or Arabic translations). 

### Ranking metrics (useful for your project)

They report ranking quality using:

* **MAP** (Mean Average Precision)
* **R-Pr** (R-Precision)
* **P@k** for multiple cutoffs: **P@5, P@10, P@20, P@50** 

This aligns well with a realistic fact-checking workflow where only top-$k$ items can be verified.

---

## UI/UX patterns worth copying

* Show text split into sentences with:

  * per-sentence **score**,
  * **color intensity** mapped to score range,
  * option to view sentences in original order or **sorted by score**. 
* Provide a dropdown to **mimic different media outlets** (different “selection strategies”). 

---

## Implementation checklist (portable into your project)

### Minimal “ClaimRank-like” backend

* [ ] Language detection (`langdetect` or equivalent) 
* [ ] Sentence splitting: NLTK for English; regex rules for Arabic 
* [ ] Tokenization:

  * [ ] English: NLTK tokenizer 
  * [ ] Arabic: Farasa segmentation 
* [ ] POS tagging + Universal POS mapping 
* [ ] Feature extractors:

  * TF–IDF, NER, sentiment, length 
  * lexicon features (bias/sentiment/assertive/subjective) 
  * LDA topics + embeddings 
* [ ] Model: 2-hidden-layer MLP (200 ReLU → 50 ReLU → sigmoid) trained with SGD 
* [ ] Ranking: sort by predicted positive probability 
* [ ] Session cache: store sentences + scores for resorting and “mimic source” modes 

### Cross-lingual path

* [ ] Build / import cross-lingual embeddings (MUSE / VecMap / other) 
* [ ] Ensure Arabic segmentation matches the embedding training assumptions (paper notes sparsity issues with unsegmented Arabic). 

---

## Source

* Jaradat et al., *ClaimRank: Detecting Check-Worthy Claims in Arabic and English*, NAACL-HLT 2018 (Demo). 
