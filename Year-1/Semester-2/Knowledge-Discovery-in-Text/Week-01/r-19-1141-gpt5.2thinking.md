# Technologies and Techniques from the Article

**For future reuse in a check-worthiness / fact-checking prioritization project**

**Source article:** *It Takes Nine to Smell a Rat: Neural Multi-Task Learning for Check-Worthiness Prediction* (RANLP 2019) 

---

## 1) What the paper solves

The paper focuses on **check-worthiness prediction**: given sentences from political debates, rank which claims should be prioritized for manual fact-checking. Instead of imitating only one fact-checking organization, it learns from **multiple fact-checkers simultaneously** using **multi-task learning**. 

This is highly reusable for projects such as:

* claim prioritization pipelines
* misinformation triage systems
* newsroom fact-checking assistants
* content moderation escalation ranking
* “what to verify first” ranking in monitoring systems

---

## 2) Core reusable idea (high value for your project)

## Multi-task learning across annotation sources

### Why it matters

Different fact-checking organizations often choose different claims, with low overlap. The paper shows that **joint learning from multiple sources** improves performance when targeting a single source as well. 

### Reusable pattern

Treat each annotation source as a separate task:

* Task 1 = PolitiFact selection
* Task 2 = FactCheck selection
* ...
* Task 9 = Washington Post selection
* Optional task = **ANY** (selected by at least one source)

This is a strong pattern for any domain where:

* labels come from different reviewers/teams
* policies differ
* agreement is partial

Examples beyond fact-checking:

* fraud review teams
* moderation policies across platforms
* medical annotators / hospitals
* legal review teams
* support ticket priority labels from different departments

---

## 3) Model architecture (reusable design)

The paper uses a **hard parameter sharing multi-task neural network** architecture. 

### Architecture components

1. **Input layer** with engineered features
2. **Shared hidden layer** (common representation across tasks)
3. **Task-specific hidden layers** (one per source/task)
4. **Task-specific output layer** with a **single sigmoid** unit per task

### Why this is useful

* shares useful signal across related tasks
* reduces overfitting (classic benefit of hard parameter sharing)
* allows source-specific behavior via task-specific branches

### Reusable design pattern (generalized)

For your project, this becomes:

* **Shared encoder** = common claim representation
* **Task heads** = source/team/domain-specific scoring heads
* **Sigmoid outputs** = independent probability of selection/relevance per task

---

## 4) Learning objective and output usage

The model outputs probabilities for each task (source), then uses these probabilities to **rank claims for prioritization**. 

### Reusable principle

Even if your downstream action is binary (“check / don’t check”), train and use the model as a **ranking system**, not only classification.

### Why ranking-first is better

In fact-checking workflows, analysts usually need:

* top 10
* top 20
* top 50 candidates

This matches how the paper evaluates the task (ranking metrics). 

---

## 5) Feature engineering techniques (very reusable)

A major contribution for practical systems is the **rich engineered feature set**. The paper combines lexical, structural, semantic, and contextual features. 

## 5.1 Lexical and syntactic features

* **TF-IDF weighted bag-of-words**
* **Part-of-speech (POS) tags**
* **Sentence length** (number of tokens)

Use when:

* baseline models are needed
* feature transparency matters
* dataset is relatively small

---

## 5.2 Entity and semantic cues

* **Named entity presence (NER)**
* **Sentiment scores**
* **Lexicon-based features** (bias, sentiment, assertiveness, subjectivity)

Why useful:

* check-worthy claims often contain entities, numbers, strong assertions, or controversial framing
* lexical cueing helps when deep contextual data is limited

---

## 5.3 Context / discourse features (very important)

The paper emphasizes features beyond the sentence itself. 

* **Sentence position** within debate/intervention
* **Segment/intervention structure**
* **Discourse relations with neighboring sentences**

### Reusable insight

For “claim importance” tasks, **context matters**:

* where the statement appears
* whether it is part of a long argument
* relation to previous/next sentences

This is directly reusable in:

* debate transcripts
* interviews
* podcasts
* parliamentary sessions
* news articles with quoted speech

---

## 5.4 Topic and embedding features

* **LDA topic features**
* **Pretrained word embeddings** (Google News embeddings mentioned)
* **Similarity to previously fact-checked claims** (appears in analysis/ablation discussion)

### Reusable insight

The paper’s ablation analysis shows **embeddings are among the most important features**, while **metadata/context features** are also very influential. 

This suggests a hybrid system is strong:

* semantic representation + metadata/context + lexical cues

---

## 6) Training setup and optimization choices

From the experiments section, the paper uses: 

* **4-fold cross-validation** (leave-one-debate-out across 4 debates)
* **3 reruns with different random seeds** (average results for stability)
* **ReLU units**
* **Shared layer size = 300**
* **SGD with Nesterov momentum**
* **100 epochs**
* Adam was tried but converged faster with slightly worse results (reported by authors)

### Reusable best practices

* **Repeat runs with multiple seeds** for neural models
* report **mean performance**, not one lucky run
* test multiple optimizers (faster ≠ better)
* use task-relevant validation (ranking metrics, not only loss)

---

## 7) Evaluation methodology (excellent for your README / project design)

The task is evaluated as **ranking**, not just classification. The paper uses IR metrics: 

* **MAP (Mean Average Precision)** — primary metric
* **R-Precision (R-Pr)**
* **Precision@k** (P@5, P@10, P@20, P@50)

### Reusable evaluation strategy

If your project ranks claims/items for human review, use:

* **MAP** for global ranking quality
* **P@k** for actual analyst workflow usefulness
* optionally **Recall@k** if missing true positives is costly

---

## 8) Baseline strategy (how to structure experiments)

The paper compares several variants, which is a very good reusable experimental design. 

### Useful baseline ladder

1. **Simple / legacy baseline** (e.g., ClaimBuster-like)
2. **Single-task neural model** (target source only)
3. **Multi-task model** (all sources)
4. **Multi-task + auxiliary ANY task**
5. **Ablation variants** (remove features / tasks)

### Reusable lesson

Adding only a generic “ANY” auxiliary task is **not enough**; modeling all source-specific outputs provided stronger gains. 

---

## 9) Error analysis techniques you can reuse

The paper includes qualitative error analysis with:

* true positives improved by multi-task learning
* false positives caused by shared-task reinforcement
* false negatives for both high-agreement and low-agreement cases 

### Reusable error-analysis framework

For your project, analyze errors by:

* **agreement level across annotators/sources**
* **entity/number presence**
* **negation/contradiction cues**
* **source/task-specific confusion**
* **context length / discourse position**

This helps determine whether the shared representation is helping or hurting specific tasks.

---

## 10) Ablation analysis patterns (very important for future projects)

The paper runs two useful ablations: 

## 10.1 Feature-group ablation

Remove one feature group at a time (embeddings, metadata, sentiment, topics, NER, etc.) and compare metric drops.

### Why this is reusable

This tells you:

* what features are genuinely useful
* what can be removed to simplify production systems
* what is robust across sources/tasks

---

## 10.2 Source/task ablation

Remove one source/task from multi-task training and measure impact on other targets.

### Why this is powerful

This reveals:

* which tasks provide useful transfer
* which tasks introduce conflict/noise
* how to design task grouping or weighting

This is applicable to any multi-annotator or multi-domain setup.

---

## 11) Practical techniques to reuse in your project (actionable checklist)

## Recommended system design (based on the paper)

* [ ] Formulate the problem as **ranking**
* [ ] Use **multi-task learning** if you have multiple label sources
* [ ] Start with **hard parameter sharing**
* [ ] Include **task-specific heads**
* [ ] Use **sigmoid outputs** per task for independent probabilities
* [ ] Combine **embeddings + metadata/context + lexical features**
* [ ] Evaluate with **MAP / P@k**
* [ ] Run **feature ablations**
* [ ] Run **task/source ablations**
* [ ] Repeat training with **multiple seeds**

---

## 12) Technologies / methods mentioned in the article (README-friendly list)

### Machine Learning / Deep Learning

* Multi-task learning (MTL) 
* Hard parameter sharing neural architecture 
* Single-task vs multi-task neural models 
* Sigmoid output units for binary per-task prediction 
* ReLU activation 
* SGD + Nesterov momentum optimization 

### NLP Features / Representations

* TF-IDF bag-of-words 
* POS tags 
* Named entities (NER presence) 
* Sentiment features / sentiment scores 
* Lexicon features (bias, assertiveness, subjectivity) 
* Structural and positional features 
* LDA topic features 
* Pretrained word embeddings 
* Discourse relation features with neighboring sentences 
* Similarity to previously fact-checked claims (in analysis/ablation discussion) 

### Evaluation / Experimentation

* 4-fold cross-validation (leave-one-debate-out) 
* Multiple random-seed reruns for stability 
* Ranking-based evaluation (MAP, R-Precision, P@k) 
* Feature ablation experiments 
* Source/task ablation experiments 
* Comparative error analysis 

---

## 13) How to adapt these ideas to a modern project (optional modernization path)

If you plan to implement a newer version, keep the **same experimental logic** but modernize the encoder:

### Keep

* multi-task setup
* ranking metrics
* source/task ablations
* context-aware features / metadata

### Replace / extend

* static word embeddings → contextual embeddings (e.g., transformer encoder)
* manually engineered lexical features → hybrid input (engineered + contextual)
* simple MTL head → weighted losses / uncertainty weighting / soft parameter sharing

The paper itself also suggests future exploration of **soft parameter sharing** for sources that do not benefit equally from shared learning (e.g., NYT in their results). 
