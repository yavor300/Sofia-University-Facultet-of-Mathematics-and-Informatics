# Technologies and Techniques Used in ClaimRank

This document outlines the architecture, algorithms, and Natural Language Processing (NLP) tools used in the ClaimRank system for automatically detecting check-worthy claims.

## 1. System Architecture
* **Model**: A web-based client-server architecture accessible via a web browser.
* **Session Management**: The extracted sentence list is converted to JSON format and stored in the user's session.
* **Scoring**: Check-worthiness scores are stored in the session object as parallel arrays alongside the sentence list.

## 2. Preprocessing Tools
The system supports both English and Arabic, utilizing different tools adapted for each language:
* **Language Detection**: Uses Python's `langdetect` library.
* **Sentence Splitting**:
  * *English*: Handled by NLTK's `sent_tokenize`.
  * *Arabic*: Handled by custom regular expressions, as Arabic uses different sentence-ending characters (e.g., the Arabic question mark).
* **Tokenization and POS Tagging**:
  * *English*: Uses NLTK's tokenizer and Part-of-Speech (POS) tagger.
  * *Arabic*: Uses the `Farasa` segmenter for tokenization, word segmentation (necessary for separating conjunctions and clitics to prevent vocabulary explosion), and POS tagging.
* **Tagset Mapping**: Since English uses the Penn Treebank tagset and Arabic uses the Farasa tagset, all POS tags are mapped to the Universal tagset for consistency.

## 3. Feature Extraction
To determine check-worthiness, the system extracts rich features based on previous research:
* **Basic Lexical Features**: TF.IDF-weighted bag of words, POS tags, and sentence length in tokens.
* **Named Entities**: Recognized using the Alchemy API.
* **Lexicons**: Uses specialized dictionaries for bias, sentiment, assertiveness, and subjectivity.
* **Structural and Contextual Features**: Includes the sentence's location within the text/debate, LDA (Latent Dirichlet allocation) topics, word embeddings, and discourse relations with neighboring sentences.

## 4. Machine Learning Model
* **Model Type**: A neural network with two hidden layers.
* **Input Layer**: Receives features representing both the claim and its surrounding context.
* **Hidden Layers**:
  * First layer: 200 neurons with a ReLU activation function.
  * Second layer: 50 neurons with a ReLU activation function.
* **Output Layer**: A single sigmoid unit that classifies the sentence as check-worthy or not.
* **Ranking**: Claims are ranked based on the probability assigned by the model that they belong to the positive class.
* **Training**: The model is trained for 100 iterations using Stochastic Gradient Descent.

## 5. Cross-Language Embeddings
To support multiple languages, several approaches for projecting monolingual embeddings into a joint vector space were evaluated:
* **VecMap**: Generates monolingual embeddings using `word2vec` (trained on TED talks) and projects them into a joint space.
* **MUSE Embeddings**: Uses Facebook's supervised MUSE model to project Arabic and English embeddings into a joint space based on TED talks corpora. This was chosen for the final system as it yielded the best overall performance.
* **Attract-Repel**: Uses pre-trained embeddings that impose monolingual synonymy and antonymy constraints.
